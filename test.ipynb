{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/iconocity/iconicity_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import torch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carica il modello CLIP\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import librosa\n",
    "\n",
    "def load_video(video_path, frame_skip=30):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    frame_count = 0\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        if frame_count % frame_skip == 0:\n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            frame_pil = Image.fromarray(frame_rgb)\n",
    "            frames.append(frame_pil)\n",
    "        frame_count += 1\n",
    "    cap.release()\n",
    "    return frames\n",
    "\n",
    "def load_audio(audio_path):\n",
    "    y, sr = librosa.load(audio_path)\n",
    "    spectrogram = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128, fmax=8000)\n",
    "    spectrogram_db = librosa.power_to_db(spectrogram, ref=np.max)\n",
    "    return spectrogram_db\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting librosa\n",
      "  Downloading librosa-0.10.2.post1-py3-none-any.whl (260 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m260.1/260.1 KB\u001b[0m \u001b[31m79.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: decorator>=4.3.0 in /root/iconocity/iconicity_env/lib/python3.10/site-packages (from librosa) (5.1.1)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in /root/iconocity/iconicity_env/lib/python3.10/site-packages (from librosa) (1.5.1)\n",
      "Requirement already satisfied: numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3 in /root/iconocity/iconicity_env/lib/python3.10/site-packages (from librosa) (1.26.4)\n",
      "Collecting soundfile>=0.12.1\n",
      "  Downloading soundfile-0.12.1-py2.py3-none-manylinux_2_31_x86_64.whl (1.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m272.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting soxr>=0.3.2\n",
      "  Downloading soxr-0.3.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hCollecting pooch>=1.1\n",
      "  Downloading pooch-1.8.2-py3-none-any.whl (64 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.6/64.6 KB\u001b[0m \u001b[31m566.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting audioread>=2.1.9\n",
      "  Downloading audioread-3.0.1-py3-none-any.whl (23 kB)\n",
      "Collecting lazy-loader>=0.1\n",
      "  Downloading lazy_loader-0.4-py3-none-any.whl (12 kB)\n",
      "Collecting msgpack>=1.0\n",
      "  Downloading msgpack-1.0.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (385 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m385.1/385.1 KB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: scipy>=1.2.0 in /root/iconocity/iconicity_env/lib/python3.10/site-packages (from librosa) (1.14.0)\n",
      "Requirement already satisfied: joblib>=0.14 in /root/iconocity/iconicity_env/lib/python3.10/site-packages (from librosa) (1.4.2)\n",
      "Collecting numba>=0.51.0\n",
      "  Downloading numba-0.60.0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m986.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:01\u001b[0m00:01\u001b[0m0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=4.1.1 in /root/iconocity/iconicity_env/lib/python3.10/site-packages (from librosa) (4.12.2)\n",
      "Requirement already satisfied: packaging in /root/iconocity/iconicity_env/lib/python3.10/site-packages (from lazy-loader>=0.1->librosa) (24.1)\n",
      "Collecting llvmlite<0.44,>=0.43.0dev0\n",
      "  Downloading llvmlite-0.43.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (43.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.9/43.9 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /root/iconocity/iconicity_env/lib/python3.10/site-packages (from pooch>=1.1->librosa) (2.32.3)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /root/iconocity/iconicity_env/lib/python3.10/site-packages (from pooch>=1.1->librosa) (4.2.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /root/iconocity/iconicity_env/lib/python3.10/site-packages (from scikit-learn>=0.20.0->librosa) (3.5.0)\n",
      "Collecting cffi>=1.0\n",
      "  Downloading cffi-1.16.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (443 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m443.9/443.9 KB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting pycparser\n",
      "  Downloading pycparser-2.22-py3-none-any.whl (117 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.6/117.6 KB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: urllib3<3,>=1.21.1 in /root/iconocity/iconicity_env/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2.2.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /root/iconocity/iconicity_env/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /root/iconocity/iconicity_env/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2024.7.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /root/iconocity/iconicity_env/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.7)\n",
      "Installing collected packages: soxr, pycparser, msgpack, llvmlite, lazy-loader, audioread, pooch, numba, cffi, soundfile, librosa\n",
      "Successfully installed audioread-3.0.1 cffi-1.16.0 lazy-loader-0.4 librosa-0.10.2.post1 llvmlite-0.43.0 msgpack-1.0.8 numba-0.60.0 pooch-1.8.2 pycparser-2.22 soundfile-0.12.1 soxr-0.3.7\n"
     ]
    }
   ],
   "source": [
    "# !pip install librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supponiamo di avere una lista di file video e audio\n",
    "video_files = [\"data/Bald_Headed_Eagle_catches_salmon.mp4\"]\n",
    "audio_files = [\"data/file_example_WAV_1MG.wav\"]\n",
    "videos = [load_video(video) for video in video_files]\n",
    "audios = [load_audio(audio) for audio in audio_files]\n",
    "\n",
    "video_embeddings = []\n",
    "audio_embeddings = []\n",
    "\n",
    "for video in videos:\n",
    "    inputs = processor(images=video, return_tensors=\"pt\", padding=True)\n",
    "    with torch.no_grad():\n",
    "        embeddings = model.get_image_features(**inputs)\n",
    "    video_embeddings.append(embeddings.mean(dim=0))  # Media degli embedding per rappresentare il video\n",
    "\n",
    "for audio in audios:\n",
    "    audio_embeddings.append(torch.from_numpy(audio.flatten()))\n",
    "\n",
    "video_embeddings = torch.stack(video_embeddings)\n",
    "audio_embeddings = torch.stack(audio_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32768])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_embeddings.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Definisci il modello\n",
    "class MIModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(MIModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, (hidden, _) = self.lstm(x.view(1, 1, -1))\n",
    "        output = self.fc(hidden[0])\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crea il modello\n",
    "model_mi = MIModel(video_embeddings.size(1), 128, audio_embeddings.size(1))\n",
    "\n",
    "# Definisci la funzione di perdita\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Definisci l'ottimizzatore\n",
    "optimizer = torch.optim.Adam(model_mi.parameters())\n",
    "\n",
    "# Crea il dataset e il dataloader\n",
    "dataset = TensorDataset(video_embeddings, audio_embeddings)\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.25863176584243774\n",
      "Epoch 2/10, Loss: 0.24095959961414337\n",
      "Epoch 3/10, Loss: 0.22759336233139038\n",
      "Epoch 4/10, Loss: 0.215243399143219\n",
      "Epoch 5/10, Loss: 0.20388157665729523\n",
      "Epoch 6/10, Loss: 0.19346177577972412\n",
      "Epoch 7/10, Loss: 0.18392232060432434\n",
      "Epoch 8/10, Loss: 0.17519310116767883\n",
      "Epoch 9/10, Loss: 0.1672051101922989\n",
      "Epoch 10/10, Loss: 0.15989616513252258\n"
     ]
    }
   ],
   "source": [
    "# Addestra il modello\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    for audio, video in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model_mi(audio)\n",
    "        loss = criterion(output, video)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{epochs}, Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "\n",
    "# ... (definizione del modello linguistico e altri parametri)\n",
    "\n",
    "def calcola_entropia(modello, dati_minacce):\n",
    "  \"\"\"Calcola l'entropia del modello linguistico.\n",
    "\n",
    "  Args:\n",
    "    modello: il modello linguistico (LSTM).\n",
    "    dati_minacce: un tensore che contiene i vettori di minaccia.\n",
    "\n",
    "  Returns:\n",
    "    L'entropia calcolata.\n",
    "  \"\"\"\n",
    "  \n",
    "  # Metti il modello in modalità di valutazione\n",
    "  modello.eval()\n",
    "\n",
    "  # Genera una distribuzione di probabilità sui segnali d'allarme predetti dal modello\n",
    "  with torch.no_grad():\n",
    "    predizioni = modello(dati_minacce) \n",
    "    probabilità_predizioni = torch.softmax(predizioni, dim=1)\n",
    "\n",
    "  # Calcola l'entropia \n",
    "  entropia = -torch.sum(probabilità_predizioni * torch.log(probabilità_predizioni), dim=1).mean()\n",
    "\n",
    "  return entropia "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crea il dataset casuale di minacce\n",
    "dati_minacce_casuali = torch.rand(len(video_embeddings), video_embeddings.size(1))\n",
    "\n",
    "# Crea il dataset reale di minacce\n",
    "dati_minacce = video_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcola l'entropia H(S)\n",
    "entropia_s = calcola_entropia(model_mi, dati_minacce_casuali)\n",
    "\n",
    "# Calcola l'entropia condizionata H(S|T)\n",
    "entropia_s_t = calcola_entropia(model_mi, dati_minacce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.0075)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(entropia_s - entropia_s_t) / entropia_s *100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2 Entropy Calculation\n",
    "    # Using the trained model, generate alarm signal predictions\n",
    "    # using a set of random threat vectors (or real threat vectors)\n",
    "    # and calculate the entropy of the probability distribution over the predicted signals.\n",
    "# 4.3 Conditional Entropy Calculation\n",
    "    # Using the trained model, generate alarm signal predictions\n",
    "    # using the real dataset of threat-alarm signal pairs\n",
    "    # and calculate the conditional entropy of the probability distribution over the predicted signals.\n",
    "# 4.4 MI Calculation\n",
    "    # Subtract the conditional entropy from the entropy to obtain the MI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iconicity_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
